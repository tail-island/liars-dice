# ディープ・ラーニングで、プレイヤー判別してみた

嘘つきサイコロではブラフを見破ることが重要で、そのためには敵がどんなやつなのかという情報が役に立ちそうな気がします。たとえば、2019年度のプロコンのサイトが公開されて、サイトにプレイヤー判定サンプルのソース・コードがなんの解説もなく置いてあって、で、「解説記事を作成中」と書いてあるのを見た場合、私をよく知る人なら「こいつはプログラムを組んだところで満足して、まだカケラも解説記事を書いていないな」と判断できます。でも、私を知らない人は、「作成中なのだから数日のうちには完成して公開されるだろう」と騙されちゃう。ほら、相手がどんなダメ人間かを知るのは重要でしょ？　あの、皆様を騙してしまって本当にすみませんでした……。

というわけで、放置していた[敵がサンプルのどれなのかを判別するプログラム](https://github.com/tail-island/liars-dice-analyzer)の解説です。ディープ・ラーニング入門を兼ねていますので、お時間があるときにでも。

## とりあえず、判別プログラムを動かしてみる

以下の手順で、プログラムを動かすことができます。

1. Python 3.6以降をインストール
2. pip install tensorflow keras funcy h5py
3. python train.py
4. python check.py

最後にどれだけ判定できたかがaccuracy = *n*で表示されます。もしaccuracy = 1.0と表示されたら、すべてを正しく判別できたということになります。

## all-games.jsonから、データを作成する

さて、ディープ・ラーニングのような機械学習では、大量の入力と出力のペアを使用して、どんな入力のときにはどんな出力をすればよいのかを学んでいきます（いわゆる教師あり学習ってやつです。教師なし学習という入力だけでの学習もありますけど、今回は対象外）。人間の場合で言えば、この人は美人、この人はそうではないってのを延々と教えこまれて、その時代のその社会が美人とかかっこいいとか判断する基準を学習して、その基準によって私が見た目で拒否され結婚できなくて週末にやることが何もなくてプログラミングしちゃっているような感じ。私は生まれる時代か社会を間違えただけなのだと思いたい……。

で、いわゆる機械学習では、この数式で出したある特徴を表現する数値がこんな判定をする場合には有効だよね、みたいな定跡があって、多くのケースでは、生データではなくその特徴的な数値を入力に使用します。たとえば音声だと、MFCC（メル周波数ケプストラム係数）とかね。身長と体重から服のサイズを予測するような単純な場合はわざわざBMI（ボディマス指数。体重を身長の二乗で割って私を肥満度1と判断するアレ）で特徴量を抽出したりはしない気もするけど、この程度の問題なら機械学習は使わないでしょう。

ところが、ディープ・ラーニングでは、生データをそのまま使って良いらしいんです。私は昔、音声解析で遊んでみたときに、[MFCCで前処理した入力を使用した場合](https://github.com/tail-island/devil-ear)と、[生データ（WAVEデータそのまま）を入力とした場合](https://github.com/tail-island/toyo-to-mimi)で精度が変わらなくて（生データの方がむしろ精度が高くなって）びっくりしました。ディープ・ラーニングでは、どんなふうに特徴を抽出すればよいのかも含めて学習してくれるんですな。

とはいえ、データが何を表しているのかは問わないディープ・ラーニングでも、データの形式には制限があります。具体的に言うと、固定長の数値の配列を入力に、固定長の数値の配列を出力にすることしかできません。だから、プレイヤー判別のための入力情報も、なんらかの形の固定長の数値の配列にする必要があります。2019年度のプロコンのプラットフォームは試合内容をall-games.jsonとして出力してくれますけど、残念ながら、このファイルの文字列をそのまま使うわけにはいきません。

というわけで、まずはデータを変換するプログラムを作りましょう。変換元データは、嘘つきサイコロのプラットフォームが出力するall-games.jsonです。データは多い方がよいだろうと考えて、引数を1000セット以上に設定して繰り返した大きなall-games.jsonを、念のために10回繰り返して10個作りました。その結果は、dataディレクトリの下にdata-*n*.jsonとして保存してあります。これでデータが準備できたので、データを変換するプログラムに取り掛かります。今回のプログラムでは、[data\_set.py](https://github.com/tail-island/liars-dice-analyzer/blob/master/data_set.py)でディープ・ラーニングに合うようにデータを変換します。

ここで重要なのは、変換先をどのようにするのかです。今回のプログラムでは、変換先（プレイヤー判別プログラムの入力）として以下を使用しました。

* 自分の、出目が☆になっているサイコロの数
* 自分の、出目が2になっているサイコロの数
* 自分の、出目が3になっているサイコロの数
* 自分の、出目が4になっているサイコロの数
* 自分の、出目が5になっているサイコロの数
* 自分の、出目が6になっているサイコロの数
* 自分以外のプレイヤーののサイコロの数の合計
* 前のプレイヤーが、2の目で宣言したかどうか
* 前のプレイヤーが、3の目で宣言したかどうか
* 前のプレイヤーが、4の目で宣言したかどうか
* 前のプレイヤーが、5の目で宣言したかどうか
* 前のプレイヤーが、6の目で宣言したかどうか
* 前のプレイヤーが、*n*以上として宣言した値
* 自分が、2の目で宣言したかどうか
* 自分が、3の目で宣言したかどうか
* 自分が、4の目で宣言したかどうか
* 自分が、5の目で宣言したかどうか
* 自分が、6の目で宣言したかどうか
* 自分が、*n*以上として宣言した値
* チャレンジしていれば1、そうでなければ0

これで1手分の情報になるわけですけど、なんとなく1手分から判断するのは難しそうだなぁと考えて、5手分を集めて長さ100の固定長配列にしました。入力でサイコロの出目そのままを数値化していないのは、出目の数字は意味的には大小を表していない（青とか赤とか、スペードとダイヤとかの大小がない場合と同じ）ためです。

このデータは、`load_data()`の中の`data_set()`の中の`create_xs()`で作成しています。

出力は、以下の長さ5の固定長配列にしました。

* プログラムがhardheadかcsharpかjavaである確率
* プログラムがfoolである確率
* プログラムがoptimistである確率
* プログラムがpessimistである確率
* プログラムがtimidである確率

csharpとjavaは、アルゴリズム的にはhardheadと同じなのでhardheadに含みました。0〜4の値を取る1つの数値にしないのは、hardheadはfoolの半分で、optimistはfoolの1.5倍だよねというような大小の関係がないことと、「多分hardheadだと思うのだけど、ひょっとしたらfoolかもしれない」という場合を表現可能にするためです（いわゆるクラス分類と呼ばれる問題では、こんな感じの出力にします。プロ野球選手の打率から年俸を予測するような回帰分析と呼ばれる問題では、年俸そのものを数値として出力する形になります）。ただし、今回のプログラムの`load_data()`の中の`data_set()`の中の`create_ys()`では、0番目の要素の値が1で他はゼロという意味で0、1番目の要素の値が1で他はゼロという意味で1といった感じで、0〜4の整数を作成しています。

とまぁこんな感じで作成した入力と出力のデータを、`load_data()`の中の`data_set()`の最後の行で、学習用と検証用に分けます。学習用データで学習した結果を、学習には使用しない検証用データで調べるためです。「プロコン課題1のオブザーバーの尾島はブサイク」と教え込むのは、「尾島はブサイク」と答えさせるためではありません。だって、そんなのはデータを検索すれば分かるのですから。学習の目的は、ハンサムかブサイクかを、学習データに含まれないあらゆる人に対しても高い精度で判別できるようにするためなんです。この目標を達成できたかを調べるには学習用以外の出データが必要で、それが検証用データなんですな。

## ニューラル・ネットワークを作る

先程の入出力データを作るだけのdata\_set.pyは、100行を超える大作でした。では、脳のニューロンの構造を模したと言われている、とても複雑なものに思えるニューラル・ネットワークを作るためのコード量がどれくらいかと言うと、今回のプログラムでは、`import`を含めても25行だけなのですよ……。

もちろんこれには秘密があって、ディープ・ラーニングのライブラリが作業の殆どを受け持ってくれているからなんです。特に、私が愛用しているKerasは素晴らしい！　関数型プログラミング・ライブラリのfuncyと組み合わせると、ニューラル・ネットワークを宣言的に記述できてとても簡単なんです。こんな感じ。

~~~ python
from data_set           import *
from funcy              import *
from keras.callbacks    import LearningRateScheduler
from keras.layers       import *
from keras.models       import Model, save_model
from keras.regularizers import l2
from operator           import getitem


def computational_graph():
    def dense(unit_size):
        return Dense(unit_size, kernel_initializer='he_normal', kernel_regularizer=l2(0.0005))

    def relu():
        return Activation('relu')

    def softmax():
        return Activation('softmax')

    return rcompose(dense(1024), relu(),
                    dense( 512), relu(),
                    dense( 256), relu(),
                    dense( 128), relu(),
                    dense(  64), relu(),
                    dense(   5), softmax())

# 関数型プログラミングで書いたので、馴染みがないとかえって分かりづらいかもしれません……。
# そんな時は薄目で見て、computational_graph()はdense→relu→dense→...→softmaxという処理を定義しているんだなぁと考えてください。
~~~
[train.py](https://github.com/tail-island/liars-dice-analyzer/blob/master/train.py)

でも、このコードを見せられても、今回がたまたま簡単なだけだと感じるかもしれません。が、ご安心ください。ディープ・ラーニングの最先端を切り開いて名を上げるというような高尚な目的がある人はともかく、私のようなディープ・ラーニングを応用して現実の問題を楽に解きたいなぁという程度なら、`dense`で実現している全結合層以外で覚えなければならないのは、あとは畳み込み層（Convolution）だけ。深層学習って、全結合層と畳み込み層を使えればほぼオッケーってくらいに道具立てが少ないんですよ。で、この少ない道具の使い方も、すごい学者さんがいろいろ考えて定跡を考案済みです。[ResNet](https://github.com/tail-island/try-residual-net)とかね。その定跡を、Kerasの楽ちんライブラリで実装するだけ。というか、コピー＆ペーストして、余裕があったらパラメーターをいじる程度という……。ほとんどの場合で、入力と出力のデータを作るコードより、ディープ・ラーニング部分のコードの方が楽なんですよ。

さて、その今回使用している全結合層ですが、これは入力となる情報に掛け算と足し算をして一つの値を出力するというものです。たくさんのデータがある場合に、どれかを重視してどれかを軽視する（場合によってはマイナスとして扱う）ことで、ある視点での評価となります。顔は気にしないとにかく身長が高ければ良いという、180cmの身長以外に取り柄がない私にとって都合が良い視点を表現するには、顔 * 0.1 + 身長 * 100とかすれば良いわけです（この0.1とか100とかのパラメーターを、大量のデータを使っていい感じに調整するのが機械学習なんです）。

（ここに絵）

でも、たった一つの視点では正しい判断はできませんよね？　たった5手分の手からプレイヤーを判別するという、今回のような複雑な処理の場合は、多くの視点からの総合的な判断が必要でしょう。自分の出目の2の数と2で宣言したかどうかを見る視点と、自分の出目の3の数と3で宣言したかどうかを見る視点と……みたいな感じです。だから今回は、とりあえず`dense(1024)`で1024個の視点を作成しました。入力である100個の数値が、`dense(1024)`を通すと様々な視点でまとめた1024個の数値になるわけです。この1024個の数値を計算するための、入力の100個の数値に掛け算する100個のパラメーターは視点単位で独立なので、ここまででも1024 * 100の102,400個の変数をいい感じに調整しなければなりません。こんな作業は手作業ではどう考えても無理なわけで、だから機械学習なのですよ。

さて、上のコードを見ると、funcyの`rcompose`で`dense()`と`relu()`を結合しています（少し分かりづらいのですけど、`dense()`も`relu()`も、その戻り値は関数です。関数型プログラミングでよく出てくる、関数を返す関数ですね）。`dense`した結果を入力に`relu`する、と考えてください。この`relu`てのはとても単純で、コードにすると以下のような処理をしています。

~~~ python
def relu(float x):
    return x if x > 0 else 0
~~~

ある視点で出力された数値が0よりも大きければその数値、そうでなければ0を返すだけです。なんでこんな無意味そうに見える処理を加えているのかと言うと、`dense`を複数層重ねることでその能力を増大させたいからなんです。でも、ただ単純に`dense`を重ねるだけだと、数学的には一層しかない場合と同じになってしまうらしい（`output = input * A * B * C`は、`X = A * B * C; output = input * X`と表現できちゃう）。この問題を防ぐためには、非線形な（グラフを描いた場合に直線にならないような）計算を入れてあげればよいみたい。非線形な処理にはいろいろあるらしいんですけど、まぁ大抵の場合は`relu`ってのを使っておけば大丈夫（昨年度の課題1の優勝の中邨さんは、Leaky ReLUってのを使っていましたが……。まぁ、あの人はプロですから）。

というわけで、`relu`を挟んで非線形にしたうえで、`dense`を重ねます。ある視点での評価と別の視点での評価を組み合わることで、新しい視点での評価を作れますよね？　`dense(512)`を重ねることで、1024個の視点での評価結果をいい感じに組み合わせた、新しい512の視点での評価結果が手に入るわけです。今回は、最終的には5個の数値がほしいので、`dense`を重ねて適当に視点を減らしていって、最後に`dense(5)`をつなげていきます。

（ここに絵）

ただ、この`dense(5)`の出力は、何らかの視点で評価した結果を表現する数値でしかないんです。1234.567とか0.000000123とかで、欲しい出力である「プログラムがhardheadかcsharpかjavaである確率」ではありません。で、単純に全出力の合計で割って比率にしてもよさそうなのですけど、ディープ・ラーニングでは、数学の`log`を使って、差を拡大して表現することが多いみたいです。で、私のような`log`なにそれ美味しいのという数学ダメダメ人間でも、`softmax`を呼び出せば、数学が得意なディープ・ラーニングのライブラリ作成者が作ってくれたイケてる処理が動いて、小さな差を大きく広げた確率の形の出力を得られるというわけです。

以上で、特定のプログラムであるかの確率を表現する5つの数値が出力されました。でも、今はまだ計算式を作成しただけで、計算式の中のパラメーターは初期値のランダムなまま。これでは役に立ちませんから、学習をさせましょう。

## 学習させる

その学習させる部分のコードは、以下の通り。

~~~ python
def main():
    (x, y), (validation_x, validation_y) = load_data()

    model = Model(*juxt(identity, computational_graph())(Input(shape=x[0].shape)))
    model.summary()
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.fit(x, y, 100, 150, validation_data=(validation_x, validation_y), callbacks=[LearningRateScheduler(partial(getitem, tuple(take(150, concat(repeat(0.001, 100), repeat(0.0005, 25), repeat(0.00025))))))])

    save_model(model, "model.h5")

# このコードも関数関数していて、特殊な人しか分からないような気が……。
# まぁ、私も、このコードは毎回コピー＆ペーストで使いまわしていて、もはや中身を読んですらいなかったのですけど。
# 今回、中身を解説するために、久しぶりにコードを読み解きました……。
~~~
[train.py](https://github.com/tail-island/liars-dice-analyzer/blob/master/train.py)

`load_data()`で読み込んだデータを、変数`x`（入力）と`y`（出力）、`validation_x`と`vlaidation_y`（検証用）に設定します。先程作成したニューラル・ネットワークを生成する`computational_graph()`の戻り地を使って、`Model`を作成します。`model.summary()`はモデルの情報の出力です。以下のような感じで、ニューラル・ネットワークの情報が出力されます。

~~~
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 100)               0
_________________________________________________________________
dense_1 (Dense)              (None, 1024)              103424
_________________________________________________________________
activation_1 (Activation)    (None, 1024)              0
_________________________________________________________________
dense_2 (Dense)              (None, 512)               524800
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0
_________________________________________________________________
dense_3 (Dense)              (None, 256)               131328
_________________________________________________________________
activation_3 (Activation)    (None, 256)               0
_________________________________________________________________
dense_4 (Dense)              (None, 128)               32896
_________________________________________________________________
activation_4 (Activation)    (None, 128)               0
_________________________________________________________________
dense_5 (Dense)              (None, 64)                8256
_________________________________________________________________
activation_5 (Activation)    (None, 64)                0
_________________________________________________________________
dense_6 (Dense)              (None, 5)                 325
_________________________________________________________________
activation_6 (Activation)    (None, 5)                 0
=================================================================
Total params: 801,029
Trainable params: 801,029
Non-trainable params: 0
_________________________________________________________________
~~~

`model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])`の部分は、どんな感じに学習させるのかを表しています。ニューラル・ネットワークのパラメーターを調整するには、まず第一に正解したか（調整は不要）間違えたか（調整が必要）が必要で、しかも、どの程度間違えたのか（大きく調整したり、小さく調整したりする）を表現する値が必要です。この、どの程度間違えたかを出力する関数を損失関数（Loss関数）と呼び、Kerasでは`loss=`で指定します。で、問題となるのはその値なのですけど、`softmax`の場合は`crossentropy`を使えば良いみたい（ちなみに回帰分析のような出力が数値の場合は、間違えた場合の差を大きくすることと、正の数にするために2乗する2乗誤差の`mean_squared_error`ってのにしておけば良いみたい）。なお、今回のコードで`crossentropy`の前に`sparse_categorical_`とついているのは、`y`の値が確率の配列ではなく、何番目が正解かを表現する整数にしているためです。

`optimizer`ってのはどんな風にパラメーターを調整していくかです。これもいろいろあるらしいのですけど、私はTensorFlowのチュートリアルで勉強した`adam`だけで今まで頑張ってきました（ResNetをやるときに`SDG`ってのを使ったけど、正直違いが分からなかった……）。最後の`accuracy`は、どの程度学習が進んだかを表現するかです。`metrics`を指定して、正答率を出力させます。

ここまでできたら、`model.fit()`で学習させます。Kerasのリファレンスを見ると、fitの引数は`fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)`となっています。`x`が入力で`y`が出力です。`batch_size`は、一度に何個のデータを使って学習させるかです。外れのデータに振り回されないように複数のデータをまとめて学習するのですけど、その単位です。`epochs`は、データ全体を何回繰り返して学習させるのかです。データを1回まわしただけで十分な精度が出ればよいのですけど、データ量が少なければそううまくは行きません。そんなときは同じデータで繰り返して学習させちゃえばよい。ただし、あまり多くの繰り返しをしてしまうと、学習データの小さな特徴に特化しすぎて、検証用データでの精度が下がってしまいます。これを過学習と呼ぶのですけど、アニメばかり見ていたら、アニメの声優の区別はつくのに、流行りのドラマの俳優の区別がつかなくなったような感じですね。この過学習が発生しない程度に大きな数値を設定します。`validation_data`は、学習では使用しない検証用のデータ。`callbacks`は、学習に何らかの処理を差し込みたい場合に使用します。今回は、`LearningRateScheduler`を使用して、学習率を少しずつ小さくするようにしています。学習率が大きいと速く学習するのですけど、スピードが速すぎて良い感じのパラメーターを飛び越しちゃいます（3.14がいい感じの値なのに、3.0と4.0を行ったり来たりしちゃう）。そこで、大きな学習率で良さげな領域まできたら、学習率を下げて慎重に良いパラメーターを探っていきます（4→3.9→3.8→3.7と少しずつ探っていく）。他のパラメーターは、ごめんなさい、使ったことが無いから分かりません……。

さて、ここまでのコードを実行すると、コンソールに以下のような出力が出てきます。

~~~
Epoch 1/150
42865/42865 [==============================] - 5s 121us/step - loss: 2.0201 - acc: 0.6074 - val_loss: 1.4560 - val_acc: 0.6420
Epoch 2/150
42865/42865 [==============================] - 5s 113us/step - loss: 1.2409 - acc: 0.6766 - val_loss: 1.0768 - val_acc: 0.7210
Epoch 3/150
42865/42865 [==============================] - 5s 108us/step - loss: 0.9505 - acc: 0.7355 - val_loss: 0.8827 - val_acc: 0.7530
~~~

`loss`は損失関数の出力の平均で`acc`は訓練用データでの精度（今回は正答率）、`val_loss`と`val_acc`は検証用データでの損失と精度です。私のIntel Core i5のラップトップPCでも1ステップを108マイクロ秒で実行できているので、1エポック分の42,865件のデータを使用した学習が42,865 * 0.000121の約5.187秒で終わっています。150エポックでも12分30秒くらい（今回はデータ読み込み部分がダサいので、起動時のデータ読み込みで数十秒余計にかかりますけど）。畳み込みを使わないのであれば、GPUがない環境でも十分にディープ・ラーニングできるというわけです。

で、最後の行で`save_model()`して、学習結果を保存しておきます。そうしないとまた学習からやり直しになっちゃいますからね。

## ディープ・ラーニング＋α

さて、先程のコードで学習をしてみると、最後の150エポック目でのログ出力は以下になりました。

~~~
Epoch 150/150
42865/42865 [==============================] - 5s 112us/step - loss: 0.1008 - acc: 0.9999 - val_loss: 0.2544 - val_acc: 0.9390
~~~

学習用データでの精度である`acc`は無視して、検証用データでの精度である`val_acc`を見てみると、正答率は0.9390となっています。100回やると6〜7回間違えちゃう。ニューラル・ネットワークを調整して更に精度を向上させることを考えてみたのですけど、ちょっとやってみた限りではあまり変わりませんでした。ディープ・ラーニングがいくら凄くても、どんな場合でも必ず正解というわけにはいかないんですよ。

というわけで、＋αを考えてみます。冷静になってみると、嘘つきサイコロでは過去の100ゲーム分のデータがもらえるので、たったの5手だけで判断する必要は無いわけです。ではどうするか？　5手で予測するのを何回も繰り返して、総合的に判断すればよい。Pythonには`statistics`というモジュールがあって、その中に最頻値を出力する`mode()`関数があります（`[1, 2, 1, 3, 1, 2]`なら`1`を返す）。これを使えば良いでしょう。

というわけで、以下のコードを書いてみました。

~~~ python
def main():
    _, (validation_x, validation_y) = load_data()

    # 答えの単位にまとめたxのタプルを作成します。
    xs = tuple(map(rcompose(partial(map, first), tuple), partition_by(second, sorted(zip(validation_x, validation_y), key=second))))

    # 学習済みモデルをロードします。
    model = load_model("model.h5")

    t = 0  # 試した回数
    c = 0  # 正解した回数

    for x, y_true in zip(xs, count()):
        # 10個単位（5手が10個で50手分）でまとめます。
        for x_batch in partition(10, x):
            # batch_sizeをlen(x)にして、複数のxに対する予測を実行します。結果は予測結果のの配列。np.argmax()は、最も大きな要素のインデックスを返します。
            y = np.array(tuple(map(np.argmax, model.predict(np.array(x_batch), batch_size=len(x)))))

            try:
                # 最頻値を取得します。
                y_mode = mode(y)
            except:
                y_mode = 9

            print('{} -> {}, {}'.format(y_true, y_mode, y))

            c += y_mode == y_true
            t += 1

    # 精度を出力します。
    print('accuracy = {}'.format(c / t))
~~~
[check.py](https://github.com/tail-island/liars-dice-analyzer/blob/master/train.py)

検証用データをいろいろやって、答え単位でまとめた配列を作って`xs`に入れます。`model.predict()`で複数の`x`をまとめて予測して、`np.argmax`で予測結果（確率の配列）を数値に変換して、`mode()`で最頻値に変換します。で、正解した回数`c`を試した回数`t`で割って、精度として出力しています。プログラムを動かしてみましょう。

~~~
0 -> 0, [0 0 0 0 0 0 0 0 0 3]
0 -> 0, [0 0 0 0 0 0 0 0 3 0]
0 -> 0, [0 3 0 0 0 0 0 0 0 0]
0 -> 0, [0 0 0 0 0 2 0 0 0 0]

（中略）

4 -> 4, [4 4 4 4 4 4 4 4 4 4]
4 -> 4, [4 4 4 4 4 4 4 4 4 4]
4 -> 4, [4 4 4 4 4 4 4 4 4 4]
4 -> 4, [4 4 4 4 4 4 4 4 4 4]
accuracy = 1.0
~~~

時々間違えているけど、何回か繰り返して最頻値を取れば、ほら、精度100%のパーフェクトです！　やりました！ プレイヤーの判別、大成功です。

## でも、よく考えてみると……

でも、よく考えてみると、嘘つきサイコロで重要なのはブラフをかますかどうかの2値分類で、サンプルのプレイヤーの分類じゃないんですよね……。で、ブラフをかますかどうかであれば、確率的に妥当な範囲を超えて宣言しているかどうかを調べるだけでよい。それだけなら、単純なif文で書けちゃう？　ここまで長々と書いたのに（読んだのに）ディープ・ラーニングは不要ってこと？

はい。少なくとも、今回のプレイヤーの判別コードだけではダメです。ただ、うまいこと応用するなら、ディープ・ラーニングが有効かもしれません。こんな解説に皆様をつき合わせちゃったので、そうだといいなぁ……。

ともあれ、ディープ・ラーニングを使えば、各種データからのプロ野球選手の年俸推定とか、過去の戦歴からの次のレースの着順の予想とか、あと、顧客情報から有望な顧客かどうかを予測するとかの、お客様のビジネスにとって有益な予測ができるかもしれません。できるとは限らない（精度が出ない場合も多い）けど、ディープ・ラーニングのプログラミングは今回紹介したようにとても簡単なので、試すだけならすぐにできちゃうんです。ぜひ、試してみてください。あと、今回はご紹介できなかった畳み込みを使えば、株価のような連続的なデータの場合に精度を高めることが可能です。工作機械のセンサーが出力する時系列情報を入力にした、障害の予測とかができるかもしれません。
